{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c41d13d",
   "metadata": {},
   "source": [
    "## PKA-KMEANS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce82865",
   "metadata": {},
   "source": [
    "Setting up the PySpark environment and creating a Spark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4feb926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieRecommendationSystem\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff393f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .appName('chapter_5') \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adefeb7d",
   "metadata": {},
   "source": [
    "Setting up the PySpark environment and creating a Spark session.\n",
    "Setting the driver memory and creating a Spark session with a specific configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f3f222",
   "metadata": {},
   "outputs": [],
   "source": [
    " data_without_header = spark.read.option(\"inferSchema\", True).\\\n",
    "option(\"header\", False).\\\n",
    "csv(\"data/kddcup.data_10_percent_corrected\")\n",
    "column_names = [ \"duration\", \"protocol_type\", \"service\", \"flag\",\n",
    "\"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\",\n",
    "\"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\",\n",
    "\"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\",\n",
    "\"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
    "\"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\",\n",
    "\"serror_rate\", \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\",\n",
    "\"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\",\n",
    "\"dst_host_count\", \"dst_host_srv_count\",\n",
    "\"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\",\n",
    "\"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\",\n",
    "\"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
    "\"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\",\n",
    "\"label\"]\n",
    "data = data_without_header.toDF(*column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba61807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "data.select(\"label\").groupBy(\"label\").count().orderBy(col(\"count\").desc()).show(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fc36b7",
   "metadata": {},
   "source": [
    "- The data is read into a DataFrame `data_without_header` without inferring the schema and\n",
    "without headers.\n",
    "- Column names are specified for the DataFrame.\n",
    "- The DataFrame `data` is created with the specified column names.\n",
    "- The `select` method is used to select the \"label\" column, followed by `groupBy` and `count`\n",
    "to count the occurrences of each label.\n",
    "- The results are ordered in descending order based on the count and displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48c26c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans, KMeansModel\n",
    "from pyspark.ml import Pipeline\n",
    "numeric_only = data.drop(\"protocol_type\", \"service\", \"flag\").cache()\n",
    "assembler = VectorAssembler().setInputCols(numeric_only.columns[:-1]).\\\n",
    "setOutputCol(\"featureVector\")\n",
    "kmeans = KMeans().setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "pipeline_model = pipeline.fit(numeric_only)\n",
    "kmeans_model = pipeline_model.stages[1]\n",
    "from pprint import pprint\n",
    "pprint(kmeans_model.clusterCenters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f9745",
   "metadata": {},
   "source": [
    "- Columns \"protocol_type\", \"service\", and \"flag\" are dropped from the DataFrame to retain only\n",
    "numeric features.\n",
    "- `VectorAssembler` is used to assemble the numeric features into a single vector column\n",
    "named \"featureVector\".\n",
    "- `KMeans` clustering algorithm is applied with \"featureVector\" as features and \"cluster\" as\n",
    "prediction column.\n",
    "- A `Pipeline` is created with stages including the `assembler` and `kmeans`.\n",
    "- The `pipeline_model` is trained on `numeric_only` data.\n",
    "- `clusterCenters()` method is used to get the centroid coordinates of the clusters found by\n",
    "KMeans and printed using `pprint`.\n",
    "- The `transform` method is used to add a \"cluster\" column to the `numeric_only` DataFrame\n",
    "using the trained `pipeline_model`.\n",
    "- The `select` method is used to select the \"cluster\" and \"label\" columns.\n",
    "- `groupBy` and `count` methods are applied to count the occurrences of each label within\n",
    "each cluster.\n",
    "- The results are ordered first by \"cluster\" and then by count in descending order, and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from random import randint\n",
    "def clustering_score(input_data, k):\n",
    "input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "assembler = VectorAssembler().setInputCols(input_numeric_only.columns[:-1]).\n",
    ",→setOutputCol(\"featureVector\")\n",
    "kmeans = KMeans().setSeed(randint(100,100000)).setK(k).\n",
    ",→setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "pipeline_model = pipeline.fit(input_numeric_only)\n",
    "kmeans_model = pipeline_model.stages[-1]\n",
    "training_cost = kmeans_model.summary.trainingCost\n",
    "return training_cost\n",
    "for k in list(range(20,100, 20)):\n",
    "print(clustering_score(numeric_only, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82923cfd",
   "metadata": {},
   "source": [
    "- The `clustering_score` function takes input data and a number of\n",
    "clusters `k` as parameters.\n",
    "- Columns \"protocol_type\", \"service\", and \"flag\" are dropped from the input\n",
    "data to retain only numeric features.\n",
    "- `VectorAssembler` is used to assemble the numeric features into a\n",
    "single vector column named \"featureVector\".\n",
    "- `KMeans` clustering algorithm is applied with \"featureVector\" as\n",
    "features, \"cluster\" as the prediction column, and a random seed.\n",
    "- A `Pipeline` is created with stages including the `assembler` and\n",
    "`kmeans`.\n",
    "- The `pipeline_model` is trained on the `input_numeric_only` data.\n",
    "- The training cost of the KMeans model is calculated using\n",
    "`kmeans_model.summary.trainingCost`.\n",
    "- The function is then called for different values of `k` ranging from 20 to 80\n",
    "in steps of 20, and the training costs are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d65ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_score_1(input_data, k):\n",
    "input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "assembler = VectorAssembler().\\\n",
    "setInputCols(input_numeric_only.columns[:-1]).setOutputCol(\"featureVector\")\n",
    "kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).\\\n",
    "setTol(1.0e-5).setPredictionCol(\"cluster\").setFeaturesCol(\"featureVector\")\n",
    "pipeline = Pipeline().setStages([assembler, kmeans])\n",
    "pipeline_model = pipeline.fit(input_numeric_only)\n",
    "kmeans_model = pipeline_model.stages[-1]\n",
    "training_cost = kmeans_model.summary.trainingCost\n",
    "return training_cost\n",
    "for k in list(range(20,101, 20)):\n",
    "    \n",
    "    print(k, clustering_score_1(numeric_only, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9670d80",
   "metadata": {},
   "source": [
    "- The `clustering_score_1` function is similar to the previous function but\n",
    "with additional parameters for maximum iterations and tolerance.\n",
    "- Columns \"protocol_type\", \"service\", and \"flag\" are dropped from the input\n",
    "data to retain only numeric features.\n",
    "- `VectorAssembler` is used to assemble the numeric features into a\n",
    "single vector column named \"featureVector\".\n",
    "- `KMeans` clustering algorithm is applied with \"featureVector\" as\n",
    "features, \"cluster\" as the prediction column, a random seed, maximum\n",
    "iterations set to 40, and a tolerance of 1.0e-5.\n",
    "- A `Pipeline` is created with stages including the `assembler` and\n",
    "`kmeans`.\n",
    "- The `pipeline_model` is trained on the `input_numeric_only` data.\n",
    "- The training cost of the KMeans model is calculated using\n",
    "`kmeans_model.summary.trainingCost`.\n",
    "- The function is then called for different values of `k` ranging from 20 to\n",
    "100 in steps of 20, and the training costs along with `k` values are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449e5598",
   "metadata": {},
   "outputs": [],
   "source": [
    ": from pyspark.ml.feature import StandardScaler\n",
    "def clustering_score_2(input_data, k):\n",
    "input_numeric_only = input_data.drop(\"protocol_type\", \"service\", \"flag\")\n",
    "assembler = VectorAssembler().\\\n",
    "setInputCols(input_numeric_only.columns[:-1]).\\\n",
    "setOutputCol(\"featureVector\")\n",
    "scaler = StandardScaler().setInputCol(\"featureVector\").\\\n",
    "setOutputCol(\"scaledFeatureVector\").\\\n",
    "setWithStd(True).setWithMean(False)\n",
    "kmeans = KMeans().setSeed(randint(100,100000)).\\\n",
    "setK(k).setMaxIter(40).\\\n",
    "setTol(1.0e-5).setPredictionCol(\"cluster\").\\\n",
    "setFeaturesCol(\"scaledFeatureVector\")\n",
    "pipeline = Pipeline().setStages([assembler, scaler, kmeans])\n",
    "pipeline_model = pipeline.fit(input_numeric_only)\n",
    "kmeans_model = pipeline_model.stages[-1]\n",
    "training_cost = kmeans_model.summary.trainingCost\n",
    "return training_cost\n",
    "for k in list(range(60, 271, 30)):\n",
    "print(k, clustering_score_2(numeric_only, k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ea776",
   "metadata": {},
   "source": [
    "- The `clustering_score_2` function is an extension of the previous\n",
    "function but includes a `StandardScaler` to standardize the features.\n",
    "- Columns \"protocol_type\", \"service\", and \"flag\" are dropped from the input\n",
    "data to retain only numeric features.\n",
    "- `VectorAssembler` is used to assemble the numeric features into a\n",
    "single vector column named \"featureVector\".\n",
    "- `StandardScaler` is applied to standardize the \"featureVector\" and\n",
    "output a \"scaledFeatureVector\".\n",
    "- `KMeans` clustering algorithm is applied with \"scaledFeatureVector\" as\n",
    "features, \"cluster\" as the prediction column, a random seed, maximum\n",
    "iterations set to 40, and a tolerance of 1.0e-5.\n",
    "- A `Pipeline` is created with stages including the `assembler`, `scaler`,\n",
    "and `kmeans`.\n",
    "- The `pipeline_model` is trained on the `input_numeric_only` data.\n",
    "- The training cost of the KMeans model is calculated using\n",
    "`kmeans_model.summary.trainingCost`.\n",
    "- The function is then called for different values of `k` ranging from 60 to\n",
    "270 in steps of 30, and the training costs along with `k` values are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd0386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "def one_hot_pipeline(input_col):\n",
    "    indexer = StringIndexer().setInputCol(input_col).setOutputCol(input_col +\"_indexed\")\n",
    "encoder = OneHotEncoder().setInputCol(input_col + \"_indexed\").\n",
    ",→setOutputCol(input_col + \"_vec\")\n",
    "pipeline = Pipeline().setStages([indexer, encoder])\n",
    "return pipeline, input_col + \"_vec\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd48f5",
   "metadata": {},
   "source": [
    "- The `one_hot_pipeline` function is defined to create a pipeline that\n",
    "includes `StringIndexer` and `OneHotEncoder` stages.\n",
    "- `StringIndexer` is used to convert the categorical variable into indices.\n",
    "- `OneHotEncoder` is used to encode the indexed categorical variable into\n",
    "a one-hot encoded vector.\n",
    "- The pipeline and the output column name for the one-hot encoded vector\n",
    "are returned by the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f560052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_score_3(input_data, k):\n",
    "proto_type_pipeline, proto_type_vec_col = one_hot_pipeline(\"protocol_type\")\n",
    "service_pipeline, service_vec_col = one_hot_pipeline(\"service\")\n",
    "flag_pipeline, flag_vec_col = one_hot_pipeline(\"flag\")\n",
    "assemble_cols = set(input_data.columns) - \\\n",
    "{\"label\", \"protocol_type\", \"service\", \"flag\"} | \\\n",
    "{proto_type_vec_col, service_vec_col, flag_vec_col}\n",
    "assembler = VectorAssembler().setInputCols(list(assemble_cols)).\n",
    ",→setOutputCol(\"featureVector\")\n",
    "scaler = StandardScaler().setInputCol(\"featureVector\").\n",
    ",→setOutputCol(\"scaledFeatureVector\").setWithStd(True).setWithMean(False)\n",
    "kmeans = KMeans().setSeed(randint(100,100000)).setK(k).setMaxIter(40).\n",
    ",→setTol(1.0e-5).setPredictionCol(\"cluster\").\n",
    ",→setFeaturesCol(\"scaledFeatureVector\")\n",
    "pipeline = Pipeline().setStages([proto_type_pipeline, service_pipeline,\n",
    ",→flag_pipeline, assembler, scaler, kmeans])\n",
    "pipeline_model = pipeline.fit(input_data)\n",
    "kmeans_model = pipeline_model.stages[-1]\n",
    "training_cost = kmeans_model.summary.trainingCost\n",
    "return training_cost\n",
    "for k in list(range(60, 271, 30)):\n",
    "print(k, clustering_score_3(data, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a336b45",
   "metadata": {},
   "source": [
    "- The `clustering_score_3` function extends the previous clustering\n",
    "function (`clustering_score_2`) to include one-hot encoding for\n",
    "categorical variables.\n",
    "- `one_hot_pipeline` is utilized to create pipelines for one-hot encoding of\n",
    "the categorical variables \"protocol_type\", \"service\", and \"flag\".\n",
    "- The assembled columns for the `VectorAssembler` are updated to\n",
    "include the one-hot encoded vectors.\n",
    "- The rest of the pipeline stages remain the same as in\n",
    "`clustering_score_2`.\n",
    "- The function is called for different values of `k` ranging from 60 to 270 in\n",
    "steps of 30, and the training costs along with `k` values are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fd782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def entropy(counts):\n",
    "values = [c for c in counts if (c > 0)]\n",
    "n = sum(values)\n",
    "p = [v/n for v in values]\n",
    "return sum([-1*(p_v) * log(p_v) for p_v in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a5851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fun\n",
    "from pyspark.sql import Window\n",
    "cluster_label = pipeline_model.\\\n",
    "transform(data).\\\n",
    "select(\"cluster\", \"label\")\n",
    "df = cluster_label.\\\n",
    "groupBy(\"cluster\", \"label\").\\\n",
    "count().orderBy(\"cluster\")\n",
    "w = Window.partitionBy(\"cluster\")\n",
    "p_col = df['count'] / fun.sum(df['count']).over(w)\n",
    "with_p_col = df.withColumn(\"p_col\", p_col)\n",
    "result = with_p_col.groupBy(\"cluster\").\\\n",
    "agg((-fun.sum(col(\"p_col\") * fun.log2(col(\"p_col\"))))\n",
    ".alias(\"entropy\"),\n",
    "fun.sum(col(\"count\"))\n",
    ".alias(\"cluster_size\"))\n",
    "result = result.withColumn('weightedClusterEntropy',fun.col('entropy') * fun.\n",
    ",→col('cluster_size'))\n",
    "weighted_cluster_entropy_avg = result.\\\n",
    "agg(fun.sum(\n",
    "col('weightedClusterEntropy'))).\\\n",
    "collect()\n",
    "weighted_cluster_entropy_avg[0][0]/data.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa78a813",
   "metadata": {},
   "source": [
    "- The `entropy` function calculates the entropy of a distribution given\n",
    "counts.\n",
    "- The entropy of each cluster is computed using the given formula.\n",
    "- The weighted cluster entropy average is calculated by multiplying the\n",
    "entropy of each cluster by the cluster size and then summing over all\n",
    "clusters.\n",
    "- Finally, the average weighted cluster entropy is computed by dividing the\n",
    "total weighted cluster entropy by the total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c81061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_pipeline_4(data, k):\n",
    "(proto_type_pipeline, proto_type_vec_col) = one_hot_pipeline(\"protocol_type\")\n",
    "(service_pipeline, service_vec_col) = one_hot_pipeline(\"service\")\n",
    "(flag_pipeline, flag_vec_col) = one_hot_pipeline(\"flag\")\n",
    "assemble_cols = set(data.columns) - {\"label\", \"protocol_type\", \"service\",\n",
    ",→\"flag\"} | {proto_type_vec_col, service_vec_col, flag_vec_col}\n",
    "assembler = VectorAssembler(inputCols=list(assemble_cols),\n",
    ",→outputCol=\"featureVector\")\n",
    "scaler = StandardScaler(inputCol=\"featureVector\",\n",
    ",→outputCol=\"scaledFeatureVector\", withStd=True, withMean=False)\n",
    "kmeans = KMeans(seed=randint(100, 100000), k=k, predictionCol=\"cluster\",\n",
    ",→featuresCol=\"scaledFeatureVector\", maxIter=40, tol=1.0e-5)\n",
    "pipeline = Pipeline(stages=[proto_type_pipeline, service_pipeline,\n",
    ",→flag_pipeline, assembler, scaler, kmeans])\n",
    "return pipeline.fit(data)\n",
    "def clustering_score_4(input_data, k):\n",
    "pipeline_model = fit_pipeline_4(input_data, k)\n",
    "cluster_label = pipeline_model.transform(input_data).select(\"cluster\",\n",
    ",→\"label\")\n",
    "df = cluster_label.groupBy(\"cluster\", \"label\").count().orderBy(\"cluster\")\n",
    "w = Window.partitionBy(\"cluster\")\n",
    "p_col = df['count'] / fun.sum(df['count']).over(w)\n",
    "with_p_col = df.withColumn(\"p_col\", p_col)\n",
    "result = with_p_col.groupBy(\"cluster\").agg(-fun.sum(col(\"p_col\") * fun.\n",
    ",→log2(col(\"p_col\"))).alias(\"entropy\"),\n",
    "fun.sum(col(\"count\")).\n",
    ",→alias(\"cluster_size\"))\n",
    "result = result.withColumn('weightedClusterEntropy', col('entropy') *\n",
    ",→col('cluster_size'))\n",
    "weighted_cluster_entropy_avg = result.agg(fun.\n",
    ",→sum(col('weightedClusterEntropy'))).collect()\n",
    "return weighted_cluster_entropy_avg[0][0] / input_data.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ef32dc",
   "metadata": {},
   "source": [
    "- The `fit_pipeline_4` function is a modification of the `fit_pipeline_3` function with\n",
    "more organized code and better readability.\n",
    "- The `clustering_score_4` function is used to compute the weighted cluster entropy\n",
    "for different values of `k` using the KMeans clustering algorithm with one-hot encoded\n",
    "categorical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eacfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_model = fit_pipeline_4(data, 180)\n",
    "count_by_cluster_label = pipeline_model.transform(data).\\\n",
    "select(\"cluster\", \"label\").\\\n",
    "groupBy(\"cluster\", \"label\").\\\n",
    "count().orderBy(\"cluster\", \"label\")\n",
    "count_by_cluster_label.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3058f4",
   "metadata": {},
   "source": [
    "The `fit_pipeline_4` function is called with `k=180` to fit the KMeans clustering\n",
    "model.\n",
    "- The `transform` method is applied to the input data using the fitted pipeline model.-\n",
    "The resulting DataFrame is grouped by \"cluster\" and \"label\" to count the number of\n",
    "occurrences of each label within each cluster and then count is displayed ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c3dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
