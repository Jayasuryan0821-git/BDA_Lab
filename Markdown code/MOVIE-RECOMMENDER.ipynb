{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2713c4",
   "metadata": {},
   "source": [
    "PKA-MOVIE-RECOMMENDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a8727",
   "metadata": {},
   "source": [
    "- Importing necessary libraries and setting up PySpark environment.\n",
    "- Creating a SparkContext object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c658ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "# findspark.init()\n",
    "import os\n",
    "import sys\n",
    "import pyspark as ps\n",
    "import warnings\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "try:\n",
    "    # create SparkContext on all CPUs available: in my case I have 4 CPUs on my laptop\n",
    "    sc = ps.SparkContext('local[*]')\n",
    "    # sqlContext = SQLContext(sc)\n",
    "    print(\"Just created a SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a615350f",
   "metadata": {},
   "source": [
    "\n",
    "- Importing necessary libraries for unit testing.\n",
    "- Defining a test case class `TestRdd`.\n",
    "- Defining a test method `test_take` to test the `take` function of an RDD.\n",
    "- Running the tests using `unittest.TextTestRunner`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import sys\n",
    "\n",
    "class TestRdd(unittest.TestCase):\n",
    "    def test_take(self):\n",
    "        input = sc.parallelize([1,2,3,4])\n",
    "        self.assertEqual([1,2,3,4], input.take(4))\n",
    "\n",
    "def run_tests():\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestRdd)\n",
    "    unittest.TextTestRunner(verbosity=1, stream=sys.stderr).run(suite)\n",
    "\n",
    "run_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9752617",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffe3009",
   "metadata": {},
   "source": [
    "\n",
    "- Importing the `json` module.\n",
    "- Defining lists `fields`, `fields2`, `fields3`, and `fields4` containing field names.\n",
    "- Defining a function `validate` to check if all fields in `fields2` are present in a line.\n",
    "- Loading a JSON file `movies.json` as an RDD.\n",
    "- Mapping each line of the RDD to a JSON object and filtering lines based on the `validate` function.\n",
    "- Caching the filtered RDD for optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4eb03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "fields = ['product_id', 'user_id', 'score', 'time']\n",
    "fields2 = ['product_id', 'user_id', 'review', 'profile_name', 'helpfulness', 'score', 'time']\n",
    "fields3 = ['product_id', 'user_id', 'time']\n",
    "fields4 = ['user_id', 'score', 'time']\n",
    "\n",
    "def validate(line):\n",
    "    for field in fields2:\n",
    "        if field not in line:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "reviews_raw = sc.textFile('data/movies.json')\n",
    "reviews = reviews_raw.map(lambda line: json.loads(line)).filter(validate)\n",
    "reviews.cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a283579",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c1779",
   "metadata": {},
   "source": [
    "\n",
    "- Grouping the reviews by `product_id` and counting the number of unique movies.\n",
    "- Grouping the reviews by `user_id` and counting the number of unique users.\n",
    "- Counting the total number of reviews.\n",
    "- Printing the total number of reviews, the number of unique movies, and the number of unique users in a formatted string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac16736",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_movies = reviews.groupBy(lambda entry: entry['product_id']).count()\n",
    "num_users = reviews.groupBy(lambda entry: entry['user_id']).count()\n",
    "num_entries = reviews.count()\n",
    "print(str(num_entries) + \" reviews of \" + str(num_movies) + \" movies by \" +\n",
    "str(num_users) + \" different people.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87907be5",
   "metadata": {},
   "source": [
    "\n",
    "- Mapping each review to a tuple where the key is the product_id and the value is 1.\n",
    "- Mapping each tuple value to a tuple with the count and 1.\n",
    "- Reducing by key to sum up the counts and the ones.\n",
    "- Filtering out the results where the count is greater than 20.\n",
    "- Mapping each tuple to a new tuple where the key is the sum of counts and ones, and the value is the product_id.\n",
    "- Sorting the results by the key in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c912b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suggestion_users = reviews.filter(lambda entry: entry['user_id'])\n",
    "#for review in Suggestion_users.collect():\n",
    "r1 = reviews.map(lambda r: ((r['product_id'],), 1))\n",
    "avg3 = r1.mapValues(lambda x: (x, 1)) \\\n",
    "          .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "avg3 = avg3.filter(lambda x: x[1][1] > 20)\n",
    "avg3 = avg3.map(lambda x: ((x[1][0] + x[1][1],), x[0])) \\\n",
    "           .sortByKey(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0420fd44",
   "metadata": {},
   "source": [
    "\n",
    "- Iterating over the first 10 elements of `avg3`.\n",
    "- Printing a formatted string containing the URL of the movie on Amazon and the number of people who watched it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219c3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in avg3.take(10):\n",
    "    print(\"http://www.amazon.com/dp/\" + movie[1][0] + \" WATCHED BY : \" +\n",
    "          str(movie[0][0]) + \" PEOPLE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248e509",
   "metadata": {},
   "source": [
    "\n",
    "- Mapping each review to a tuple where the key is the user_id and the value is 1.\n",
    "- Mapping each tuple value to a tuple with the count and 1.\n",
    "- Reducing by key to sum up the counts and the ones.\n",
    "- Filtering out the results where the count is greater than 20.\n",
    "- Mapping each tuple to a new tuple where the key is the sum of counts and ones, and the value is the user_id.\n",
    "- Sorting the results by the key in descending order.\n",
    "- Iterating over the first 10 elements of `avg2`.\n",
    "- Printing a formatted string containing the URL of the movie on Amazon and the number of movies watched by the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beded5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = reviews.map(lambda ru: ((ru['user_id'],), 1))\n",
    "avg2 = r2.mapValues(lambda x: (x, 1)) \\\n",
    "          .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "avg2 = avg2.filter(lambda x: x[1][1] > 20)\n",
    "avg2 = avg2.map(lambda x: ((x[1][0] + x[1][1],), x[0])) \\\n",
    "           .sortByKey(ascending=False)\n",
    "for movie in avg2.take(10):\n",
    "    print(\"http://www.amazon.com/dp/\" + movie[1][0] + \" WATCHED : \" +\n",
    "          str(movie[0][0]) + \" MOVIES\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12cd27e",
   "metadata": {},
   "source": [
    "\n",
    "- Filtering reviews to find entries where \"George\" is in the profile name.\n",
    "- Printing the count of filtered entries.\n",
    "- Iterating over the filtered reviews and printing information about each review, including rating, helpfulness, Amazon product link, summary, and review text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2801323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Has someone written a review?\n",
    "filtered = reviews.filter(lambda entry: \"George\" in entry['profile_name'])\n",
    "print(\"Found \" + str(filtered.count()) + \" entries.\\n\")\n",
    "for review in filtered.collect():\n",
    "    print(\"Rating: \" + str(review['score']) + \" and helpfulness: \" +\n",
    "          review['helpfulness'])\n",
    "    print(\"http://www.amazon.com/dp/\" + review['product_id'])\n",
    "    print(review['summary'])\n",
    "    print(review['review'])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f15d68",
   "metadata": {},
   "source": [
    "\n",
    "- Mapping each review to a dictionary containing the score and the time converted to a datetime object.\n",
    "- Importing necessary libraries for data visualization.\n",
    "- Defining a function `parser` to parse datetime values.\n",
    "- Sampling the `timeseries_rdd` to reduce data size for plotting.\n",
    "- Converting the sampled RDD to a DataFrame.\n",
    "- Printing the first 3 rows of the DataFrame.\n",
    "- Converting the 'score' column to float64 type.\n",
    "- Setting the 'time' column as the index of the DataFrame.\n",
    "- Resampling the 'score' column by year ('Y'), month ('M'), and quarter ('Q') and plotting the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5994335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timeseries_rdd = reviews.map(lambda entry: {'score': entry['score'],\n",
    "                                             'time': datetime.fromtimestamp(entry['time'])})\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "sample = timeseries_rdd.sample(withReplacement=False, fraction=20000.0/num_entries, seed=1134)\n",
    "\n",
    "timeseries = pd.DataFrame(sample.collect(), columns=['score', 'time'])\n",
    "print(timeseries.head(3))\n",
    "\n",
    "timeseries.score = timeseries.score.astype('float64')\n",
    "#timeseries.time = timeseries.time.astype('datetime64')\n",
    "timeseries.set_index('time', inplace=True)\n",
    "\n",
    "Rsample = timeseries.score.resample('Y').count()\n",
    "Rsample.plot()\n",
    "\n",
    "Rsample2 = timeseries.score.resample('M').count()\n",
    "Rsample2.plot()\n",
    "\n",
    "Rsample3 = timeseries.score.resample('Q').count()\n",
    "Rsample3.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9727d6a",
   "metadata": {},
   "source": [
    "\n",
    "- Iterating over the first 4 elements of the `avg` RDD.\n",
    "- Creating a bar plot for each movie with its average rating.\n",
    "- Adding title, xlabel, and ylabel to the plot.\n",
    "- Displaying each plot individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a79576",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in avg.take(4):\n",
    "    plt.bar(movie[1][0], movie[0][0])\n",
    "    plt.title('Histogram of \\'AVERAGE RATING OF MOVIE\\'')\n",
    "    plt.xlabel('MOVIE')\n",
    "    plt.ylabel('AVGRATING')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29baeec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in avg2.take(3):\n",
    "    plt.bar(movie[1][0], movie[0][0])\n",
    "    plt.title('Histogram of \\'NUMBER OF MOVIES REVIEWED BY USER\\'')\n",
    "    plt.xlabel('USER')\n",
    "    plt.ylabel('MOVIE COUNT')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e887b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in avg3.take(4):\n",
    "    plt.bar(movie[1][0], movie[0][0])\n",
    "    plt.title('Histogram of \\'MOVIES REVIEWED BY NUMBER OF USERS\\'')\n",
    "    plt.xlabel('MOVIE')\n",
    "    plt.ylabel('USER COUNT')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2202b",
   "metadata": {},
   "source": [
    "\n",
    "- Importing necessary modules and libraries for collaborative filtering with ALS.\n",
    "- Defining a function `get_hash` to hash strings.\n",
    "- Mapping each review to a tuple containing hashed user_id, hashed product_id, and rating.\n",
    "- Splitting the data into training and testing sets based on a hash-based criteria.\n",
    "- Caching the training data for optimization.\n",
    "- Printing the number of samples in the training and testing sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import ALS\n",
    "from numpy import array\n",
    "import hashlib\n",
    "import math\n",
    "\n",
    "def get_hash(s):\n",
    "    return int(hashlib.sha1(s).hexdigest(), 16) % (10 ** 8)\n",
    "\n",
    "# Input format: [user, product, rating]\n",
    "ratings = reviews.map(lambda entry: tuple([get_hash(entry['user_id'].encode('utf-8')),\n",
    "                                           get_hash(entry['product_id'].encode('utf-8')),\n",
    "                                           int(entry['score'])]))\n",
    "\n",
    "train_data = ratings.filter(lambda entry: ((entry[0] + entry[1]) % 10) >= 2)\n",
    "test_data = ratings.filter(lambda entry: ((entry[0] + entry[1]) % 10) < 2)\n",
    "train_data.cache()\n",
    "\n",
    "print(\"Number of train samples: \" + str(train_data.count()))\n",
    "print(\"Number of test samples: \" + str(test_data.count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758480dc",
   "metadata": {},
   "source": [
    "\n",
    "- Building a recommendation model using Alternating Least Squares (ALS) with specified parameters: `rank` and `numIterations`.\n",
    "- Defining a function `convertToFloat` to convert lines to float values.\n",
    "- Mapping test data to contain only user and product IDs.\n",
    "- Making predictions using the trained ALS model on the test data.\n",
    "- Joining true ratings with predicted ratings.\n",
    "- Calculating Mean Squared Error (MSE) for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6938870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the recommendation model using Alternating Least Squares\n",
    "from math import sqrt\n",
    "\n",
    "rank = 20\n",
    "numIterations = 20\n",
    "model = ALS.train(train_data, rank, numIterations)\n",
    "\n",
    "def convertToFloat(lines):\n",
    "    returnedLine = []\n",
    "    for x in lines:\n",
    "        returnedLine.append(float(x))\n",
    "    return returnedLine\n",
    "\n",
    "# Evaluate the model on test data\n",
    "unknown = test_data.map(lambda entry: (int(entry[0]), int(entry[1])))\n",
    "predictions = model.predictAll(unknown).map(lambda r: ((int(r[0]), int(r[1])), r[2]))\n",
    "\n",
    "true_and_predictions = test_data.map(lambda r: ((int(r[0]), int(r[1])), r[2])).join(predictions)\n",
    "\n",
    "MSE = true_and_predictions.map(lambda r: (int(r[1][0]) - int(r[1][1]))**2).reduce(lambda x, y: x + y) / true_and_predictions.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_and_predictions.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023441d2",
   "metadata": {},
   "source": [
    "\n",
    "- Filtering reviews to obtain good reviews with a score of 5.0 and bad reviews with a score of 1.0.\n",
    "- Splitting the review texts into individual words and mapping each word to a tuple with a count of 1.\n",
    "- Reducing by key to count the occurrences of each word.\n",
    "- Filtering out words with occurrences less than `min_occurrences`.\n",
    "- Calculating the total number of good and bad words.\n",
    "- Calculating the frequency of each word in good and bad reviews by dividing the count of occurrences by the total number of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2175fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_occurrences = 10\n",
    "\n",
    "good_reviews = reviews.filter(lambda line: line['score'] == 5.0)\n",
    "bad_reviews = reviews.filter(lambda line: line['score'] == 1.0)\n",
    "\n",
    "good_words = good_reviews.flatMap(lambda line: line['review'].split(' '))\n",
    "num_good_words = good_words.count()\n",
    "good_words = good_words.map(lambda word: (word.strip(), 1)) \\\n",
    "                       .reduceByKey(lambda a, b: a + b) \\\n",
    "                       .filter(lambda word_count: word_count[1] > min_occurrences)\n",
    "\n",
    "bad_words = bad_reviews.flatMap(lambda line: line['review'].split(' '))\n",
    "num_bad_words = bad_words.count()\n",
    "bad_words = bad_words.map(lambda word: (word.strip(), 1)) \\\n",
    "                     .reduceByKey(lambda a, b: a + b) \\\n",
    "                     .filter(lambda word_count: word_count[1] > min_occurrences)\n",
    "\n",
    "# Calculate the word frequencies\n",
    "frequency_good = good_words.map(lambda word: ((word[0],), float(word[1]) / num_good_words))\n",
    "frequency_bad = bad_words.map(lambda word: ((word[0],), float(word[1]) / num_bad_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92472383",
   "metadata": {},
   "source": [
    "\n",
    "- Joining the word frequencies of good and bad reviews.\n",
    "- Calculating the relative difference of each word frequency in the good and bad reviews.\n",
    "- Sorting the dataset to get the most significant expressions for characterizing either a positively or negatively rated movie.\n",
    "- Defining a function `relative_difference` to calculate the relative difference between two values.\n",
    "- Mapping each word with its relative difference and sorting the result in descending order.\n",
    "- Taking the top 50 significant expressions for characterizing movie reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the word frequencies of the good and bad reviews\n",
    "joined_frequencies = frequency_good.join(frequency_bad)\n",
    "\n",
    "# Calculate the relative difference of each word frequency in the good and bad reviews\n",
    "import math\n",
    "\n",
    "def relative_difference(a, b):\n",
    "    return math.fabs(a - b) / a\n",
    "\n",
    "result = joined_frequencies.map(lambda f: ((relative_difference(f[1][0], f[1][1]),), f[0][0])) \\\n",
    "                           .sortByKey(ascending=False)\n",
    "\n",
    "result.take(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d9b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for movie in result.take(7):\n",
    "    plt.bar(movie[1], movie[0][0])\n",
    "\n",
    "    plt.title('Histogram of \\'SENTIMENT ANALYSIS\\'')\n",
    "    plt.xlabel('WORD')\n",
    "    plt.ylabel('NUMBER OF OCCURRENCES')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
