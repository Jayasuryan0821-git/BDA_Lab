{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f59306-74e7-4224-8a92-18b54e101a3a",
   "metadata": {},
   "source": [
    "### PKA-ENTITY-RESOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b054d6-3d87-4ae5-a883-a0ac425656b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370e5220-ea9a-483f-b7d6-926e6073bb84",
   "metadata": {},
   "source": [
    "- This code imports necessary libraries and sets up the environment for PySpark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e01e2-cfe6-4aee-aec5-a9695cc4dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").appName('chapter_2').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67a7835-617b-49c4-a318-cbcff4d977e3",
   "metadata": {},
   "source": [
    " - This code initializes a SparkSession with specific configurations, setting the driver memory to 16 GB and specifying the application name as 'chapter_2'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab7cb9-3634-4c35-a2be-042ade92153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = spark.read.csv(\"data/linkage/donation/block_1/block_1.csv\")\n",
    "prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e25425-ed51-4a2a-83a0-0b6b43aa1234",
   "metadata": {},
   "source": [
    "- This code reads a CSV file named \"block_1.csv\" located in the \"data/linkage/donation/block_1/\" directory into a DataFrame named `prev`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768fbc32-be0f-4f83-9476-40eb353fe9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dd58ea-12b6-44da-b1c5-bcb0012ce20b",
   "metadata": {},
   "source": [
    "- This code displays the first two rows of the DataFrame `prev`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22069812-313e-4b84-83c0-e0a9aa73f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = spark.read.option(\"header\", \"true\").option(\"nullValue\", \"?\").\\\n",
    "option(\"inferSchema\", \"true\").csv(\"data/linkage/donation/block_1/\n",
    "â†ªblock_1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad1d944-e4c4-4005-a352-770d24cd3305",
   "metadata": {},
   "source": [
    "- This code reads a CSV file named \"block_1.csv\" located in the \"data/linkage/donation/block_1/\" directory into a DataFrame named `parsed`.\n",
    "- It sets options such as treating the first row as the header, specifying \"?\" as the null value, and inferring the schema automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8373b6-93bd-407c-aeee-0b4dcce14b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.printSchema()\n",
    "parsed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95dbe1-55a2-4042-9d0f-bf25a2cbb32d",
   "metadata": {},
   "source": [
    "- This code prints the schema of the DataFrame `parsed` and displays the first five rows of the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4acc09a-9135-451a-a7de-036c4e572867",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0155c2de-efde-4c6e-a2d5-069b382c810f",
   "metadata": {},
   "source": [
    "- This code counts the number of rows in the DataFrame `parsed`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd623c7b-f719-4e9e-a38e-06f63bb4d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35eed225-06f8-4219-81c4-ba8ee7ab4db2",
   "metadata": {},
   "source": [
    "- This code caches the DataFrame `parsed` into memory for faster access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655b59bf-9614-4768-b701-d04ac313cbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "parsed.groupBy(\"is_match\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2705ca-666c-4716-94fb-ea34d52619f9",
   "metadata": {},
   "source": [
    "- This code groups the DataFrame `parsed` by the \"is_match\" column and counts the occurrences of each unique value.\n",
    "- It then orders the results in descending order based on the count.\n",
    "- Finally, it displays the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd158a-ecba-4b7c-bf63-bc73e472cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.createOrReplaceTempView(\"linkage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a92b5d-7290-49b1-adc8-b50ddbe8a096",
   "metadata": {},
   "source": [
    "- This code creates a temporary view named \"linkage\" for the DataFrame `parsed`, allowing SQL queries to be executed on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a313a0-025a-4209-8f38-d107fb1916e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT is_match, COUNT(*) cnt\n",
    "FROM linkage\n",
    "GROUP BY is_match\n",
    "ORDER BY cnt DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590650c9-a04b-4da7-83cd-83f23be72858",
   "metadata": {},
   "source": [
    "- This code executes a SQL query using Spark SQL. It selects the \"is_match\" column and counts the occurrences of each unique value from the \"linkage\" temporary view.\n",
    "- The results are then ordered in descending order based on the count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2957f-54fa-4b36-8d66-a917ec1158f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = parsed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa2079-8857-4d0d-a790-40cab5bf441c",
   "metadata": {},
   "source": [
    "- This code generates summary statistics for the DataFrame `parsed`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bd33f8-5efd-4646-a681-d879586b9c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.select(\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7166f-4dcf-4552-ada8-f84b1f9a22ac",
   "metadata": {},
   "source": [
    "- This code selects specific columns (\"summary\", \"cmp_fname_c1\", \"cmp_fname_c2\") from the summary DataFrame generated earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a01ffd-fc9d-44ca-b76d-6fda604d06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = parsed.where(\"is_match = true\")\n",
    "match_summary = matches.describe()\n",
    "misses = parsed.filter(col(\"is_match\") == False)\n",
    "miss_summary = misses.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a4a67f-58af-4eb9-954d-0e84c45cb465",
   "metadata": {},
   "source": [
    "- This code filters the DataFrame `parsed` to obtain rows where the \"is_match\" column is equal to `true`, storing them in a new DataFrame called `matches`.\n",
    "- It then generates summary statistics for the `matches` DataFrame and stores them in `match_summary`.\n",
    "- Similarly, it filters `parsed` to obtain rows where the \"is_match\" column is equal to `false`, storing them in a new DataFrame called `misses`.\n",
    "- It then generates summary statistics for the `misses` DataFrame and stores them in `miss_summary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701b2d8-8bc1-47cc-813b-80e2d34c67d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p = summary.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f504e-624d-4059-b33b-b969367b1950",
   "metadata": {},
   "source": [
    "- This code converts the summary DataFrame `summary` to a Pandas DataFrame `summary_p`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ab415-8fe4-4f82-a540-1d400f56d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p.head()\n",
    "summary_p.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc05256-b4e8-4a34-9892-69bef29a5aac",
   "metadata": {},
   "source": [
    "- This code displays the first few rows of the Pandas DataFrame `summary_p`.\n",
    "- It also shows the shape of the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88559eaa-35ff-47d4-8b02-d2b0c749b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_p = summary_p.set_index('summary').transpose().reset_index()\n",
    "summary_p = summary_p.rename(columns={'index':'field'})\n",
    "summary_p = summary_p.rename_axis(None, axis=1)\n",
    "summary_p.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19a3b0d-6006-4d4b-bcb4-fdbfeecd5cb3",
   "metadata": {},
   "source": [
    "- This code performs several transformations on the Pandas DataFrame `summary_p`:\n",
    "  - It sets the index of the DataFrame to the 'summary' column, transposes the DataFrame, and resets the index.\n",
    "  - It renames the 'index' column to 'field'.\n",
    "  - It removes the axis name.\n",
    "- Finally, it displays the shape of the transformed DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171e67e-aca1-4cf9-ba37-46a1af10e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryT = spark.createDataFrame(summary_p)\n",
    "summaryT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4186de54-582d-4a52-bdac-640f543404a4",
   "metadata": {},
   "source": [
    "- This code creates a Spark DataFrame `summaryT` from the transformed Pandas DataFrame `summary_p`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba69b3-b7f9-4d38-be18-541e9c1f8455",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryT.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308f273-cdab-4fa4-a14b-44ec5547abae",
   "metadata": {},
   "source": [
    "- This code prints the schema of the Spark DataFrame `summaryT`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4205d-82cc-452a-8512-809004e55f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "for c in summaryT.columns:\n",
    "    if c == 'field':\n",
    "        continue\n",
    "    summaryT = summaryT.withColumn(c, summaryT[c].cast(DoubleType()))\n",
    "\n",
    "summaryT.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ec2ff-ad3f-4f92-8f47-0a06fcb76328",
   "metadata": {},
   "source": [
    "- This code iterates through the columns of the Spark DataFrame `summaryT`.\n",
    "- For each column (except 'field'), it casts the values to DoubleType.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a3668d-fdbb-40c3-a318-bf9b998d32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def pivot_summary(desc):\n",
    "    # convert to pandas dataframe\n",
    "    desc_p = desc.toPandas()\n",
    "    # transpose\n",
    "    desc_p = desc_p.set_index('summary').transpose().reset_index()\n",
    "    desc_p = desc_p.rename(columns={'index':'field'})\n",
    "    desc_p = desc_p.rename_axis(None, axis=1)\n",
    "    # convert to Spark dataframe\n",
    "    descT = spark.createDataFrame(desc_p)\n",
    "    # convert metric columns to double from string\n",
    "    for c in descT.columns:\n",
    "        if c == 'field':\n",
    "            continue\n",
    "        else:\n",
    "            descT = descT.withColumn(c, descT[c].cast(DoubleType()))\n",
    "    return descT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92df6187-2e63-441e-9204-ff804fc1e5bd",
   "metadata": {},
   "source": [
    "- This code defines a function `pivot_summary` that takes a Spark DataFrame `desc` as input.\n",
    "- It converts the input DataFrame to a Pandas DataFrame, transposes it, and converts it back to a Spark DataFrame.\n",
    "- Then, it casts the metric columns to DoubleType.\n",
    "- Finally, it returns the transformed Spark DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d63a7-cf73-4327-843a-09e503f735f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_summaryT = pivot_summary(match_summary)\n",
    "miss_summaryT = pivot_summary(miss_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5879751f-5f88-4ad8-92d1-13a5cab2a0ed",
   "metadata": {},
   "source": [
    "- This code applies the `pivot_summary` function to the `match_summary` and `miss_summary` DataFrames, transforming them into new DataFrames `match_summaryT` and `miss_summaryT`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9217ffc8-b491-443f-9749-554dddb86947",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_summaryT.createOrReplaceTempView(\"match_desc\")\n",
    "miss_summaryT.createOrReplaceTempView(\"miss_desc\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT a.field, a.count + b.count total, a.mean - b.mean delta\n",
    "FROM match_desc a INNER JOIN miss_desc b ON a.field = b.field\n",
    "WHERE a.field NOT IN (\"id_1\", \"id_2\")\n",
    "ORDER BY delta DESC, total DESC\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf46823-bda4-4470-b276-b47964946416",
   "metadata": {},
   "source": [
    "- This code creates temporary views \"match_desc\" and \"miss_desc\" for the DataFrames `match_summaryT` and `miss_summaryT`, respectively.\n",
    "- It then executes a SQL query that joins the two views on the \"field\" column, calculates the total count for each field, and computes the difference in mean values between matched and mismatched records.\n",
    "- The query filters out fields \"id_1\" and \"id_2\" and orders the results by the difference in means in descending order, followed by the total count in descending order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f50a6f-9f52-4cf1-b5b8-7d96b593c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features = [\"cmp_lname_c1\", \"cmp_plz\", \"cmp_by\", \"cmp_bd\", \"cmp_bm\"]\n",
    "sum_expression = \" + \".join(good_features)\n",
    "sum_expression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebf01cf-c76d-451e-9bd6-59ef5e92d564",
   "metadata": {},
   "source": [
    "- This code defines a list of good features named `good_features`.\n",
    "- It then constructs a sum expression by joining the elements of `good_features` with the '+' operator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5185f76-044d-4106-9efd-fff156a1b5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
