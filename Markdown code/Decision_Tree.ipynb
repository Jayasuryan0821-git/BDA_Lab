{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194496f5",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946370b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe930e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"16g\").\n",
    ",→appName('chapter_4').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90d287",
   "metadata": {},
   "source": [
    "• Importing necessary libraries and setting up PySpark environment.\n",
    "• Creating a Spark Context object.\n",
    "Configuring the Spark session to allocate 16 gigabytes of memory to the driver and\n",
    "setting the application name to 'chapter_4' for the movie recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d5004",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_without_header = spark.read.option(\"inferSchema\", True)\\\n",
    ".option(\"header\", False).csv(\"data/covtype.data\")\n",
    "data_without_header.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b1b2e3",
   "metadata": {},
   "source": [
    "-Reading CSV Data**: The code reads a CSV file without a header and infers the\n",
    "schema.\n",
    "- Printing Schema**: It prints the schema of the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf55eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "colnames = [\"Elevation\", \"Aspect\", \"Slope\", \\\n",
    "\"Horizontal_Distance_To_Hydrology\", \\\n",
    "\"Vertical_Distance_To_Hydrology\", \"Horizontal_Distance_To_Roadways\",\n",
    ",→\\\n",
    "\"Hillshade_9am\", \"Hillshade_Noon\", \"Hillshade_3pm\", \\\n",
    "\"Horizontal_Distance_To_Fire_Points\"] + \\\n",
    "[f\"Wilderness_Area_{i}\" for i in range(4)] + \\\n",
    "[f\"Soil_Type_{i}\" for i in range(40)] + \\\n",
    "[\"Cover_Type\"]\n",
    "data = data_without_header.toDF(*colnames).\\\n",
    "withColumn(\"Cover_Type\",\n",
    "col(\"Cover_Type\").cast(DoubleType()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e30079b",
   "metadata": {},
   "source": [
    "- Importing Libraries: The code imports necessary libraries for defining data types and\n",
    "working with columns.\n",
    "- Column Names: It defines a comprehensive list of column names for the dataset\n",
    "including the ones for \"Wilderness_Area\" and \"Soil_Type\".\n",
    "- Data Conversion: The code converts the \"Cover_Type\" column to `DoubleType`.\n",
    "- Fetching Data: It retrieves the first row of the DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8b120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = data.randomSplit([0.9, 0.1])\n",
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6d11a",
   "metadata": {},
   "source": [
    "- Data Splitting: The code splits the dataset into training and testing sets with a 90-10\n",
    "ratio.\n",
    "- Caching Data: It caches the training and testing datasets to optimize performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd6a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "input_cols = colnames[:-1]\n",
    "vector_assembler = VectorAssembler(inputCols=input_cols,\n",
    "outputCol=\"featureVector\")\n",
    "assembled_train_data = vector_assembler.transform(train_data)\n",
    "assembled_train_data.select(\"featureVector\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8e5c0a",
   "metadata": {},
   "source": [
    "- The code imports the `VectorAssembler` class from `pyspark.ml.feature`.\n",
    "- It defines `input_cols` by selecting all column names except the last one from\n",
    "`colnames`.\n",
    "- `VectorAssembler` is initialized with `inputCols` set to `input_cols` and\n",
    "`outputCol` set to \"featureVector\".\n",
    "- The `transform` method is used to transform `train_data` using the\n",
    "`vector_assembler`.\n",
    "- Finally, the \"featureVector\" column is selected and displayed using `show` method\n",
    "with `truncate=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "classifier = DecisionTreeClassifier(seed = 1234, labelCol=\"Cover_Type\",\n",
    "featuresCol=\"featureVector\",\n",
    "predictionCol=\"prediction\")\n",
    "model = classifier.fit(assembled_train_data)\n",
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c672cf3",
   "metadata": {},
   "source": [
    "- The code imports the `DecisionTreeClassifier` class from\n",
    "`pyspark.ml.classification`.\n",
    "- `DecisionTreeClassifier` is initialized with parameters:\n",
    " - `seed` set to 1234\n",
    " - `labelCol` set to \"Cover_Type\"\n",
    " - `featuresCol` set to \"featureVector\"\n",
    " - `predictionCol` set to \"prediction\"\n",
    "- The `fit` method is used to train the decision tree model on `assembled_train_data`.\n",
    "- The `toDebugString` method is used to print a debug string representation of the\n",
    "trained decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c4fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(model.featureImportances.toArray(),\n",
    "index=input_cols, columns=['importance']).\\\n",
    "sort_values(by=\"importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e5b57",
   "metadata": {},
   "source": [
    "- The code imports the `pandas` library as `pd`.\n",
    "- The `featureImportances` attribute of the `model` is converted to a NumPy array\n",
    "using `toArray()`.\n",
    "- A pandas DataFrame is created with the feature importances, using `input_cols` as\n",
    "the index and 'importance' as the column name.\n",
    "- The DataFrame is sorted in descending order based on the 'importance' column using\n",
    "`sort_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80acbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(assembled_train_data)\n",
    "predictions.select(\"Cover_Type\", \"prediction\", \"probability\").\\\n",
    "show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce9d90",
   "metadata": {},
   "source": [
    "- The `transform` method is used to make predictions on `assembled_train_data`\n",
    "using the trained `model`.\n",
    "- The `select` method is used to display the actual \"Cover_Type\", predicted\n",
    "\"prediction\", and \"probability\" columns from the `predictions` DataFrame.\n",
    "- The `show` method displays the first 10 rows of the selected columns with\n",
    "`truncate=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758eb94",
   "metadata": {},
   "outputs": [],
   "source": [
    ": from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Cover_Type\",\n",
    "predictionCol=\"prediction\")\n",
    "evaluator.setMetricName(\"accuracy\").evaluate(predictions)\n",
    "evaluator.setMetricName(\"f1\").evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b00e1",
   "metadata": {},
   "source": [
    "- The code imports the `MulticlassClassificationEvaluator` class from\n",
    "`pyspark.ml.evaluation`.\n",
    "- `MulticlassClassificationEvaluator` is initialized with parameters:\n",
    " - `labelCol` set to \"Cover_Type\"\n",
    " - `predictionCol` set to \"prediction\"\n",
    "- The `setMetricName` method is used to set the metric to \"accuracy\" and then\n",
    "evaluate the predictions.\n",
    "- The `setMetricName` method is used again to set the metric to \"f1\" and then evaluate\n",
    "the predictions, storing the results in `accuracy` and `f1_score` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f7132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = predictions.groupBy(\"Cover_Type\").\\\n",
    "pivot(\"prediction\", range(1,8)).count().\\\n",
    "na.fill(0.0).\\\n",
    "orderBy(\"Cover_Type\")\n",
    "confusion_matrix.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadd8ad3",
   "metadata": {},
   "source": [
    "- The `groupBy` method groups the `predictions` DataFrame by \"Cover_Type\".\n",
    "- `pivot` is used to pivot the data based on \"prediction\" values ranging from 1 to 7\n",
    "(assuming there are 7 classes).\n",
    "- The `count` method counts the occurrences of each combination of \"Cover_Type\"\n",
    "and \"prediction\".\n",
    "- `na.fill(0.0)` is used to fill any null values with 0.0.\n",
    "- `orderBy(\"Cover_Type\")` sorts the confusion matrix by \"Cover_Type\".\n",
    "- The `show` method displays the resulting confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b752ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "def class_probabilities(data):\n",
    "total = data.count()\n",
    "return data.groupBy(\"Cover_Type\").count().\\\n",
    "orderBy(\"Cover_Type\").\\\n",
    "select(col(\"count\").cast(DoubleType())).\\\n",
    "withColumn(\"count_proportion\", col(\"count\")/total).\\\n",
    "select(\"count_proportion\").collect()\n",
    "train_prior_probabilities = class_probabilities(train_data)\n",
    "test_prior_probabilities = class_probabilities(test_data)\n",
    "train_prior_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce696f",
   "metadata": {},
   "source": [
    "- The function `class_probabilities` calculates the prior probabilities of each class in\n",
    "the dataset.\n",
    "- The total count of data is obtained using `data.count()`.\n",
    "- The `groupBy` method groups the data by \"Cover_Type\" and counts the occurrences\n",
    "of each class.\n",
    "- The counts are ordered by \"Cover_Type\" and the proportion of each class is calculated\n",
    "by dividing the count by the total count.\n",
    "- The resulting prior probabilities for the training data are computed and stored in\n",
    "`train_prior_probabilities`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prior_probabilities = [p[0] for p in train_prior_probabilities]\n",
    "test_prior_probabilities = [p[0] for p in test_prior_probabilities]\n",
    "sum([train_p * cv_p for train_p, cv_p in zip(train_prior_probabilities,\n",
    "test_prior_probabilities)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1b8881",
   "metadata": {},
   "source": [
    "The prior probabilities for the training and test datasets are extracted from\n",
    "`train_prior_probabilities` and `test_prior_probabilities`, respectively.\n",
    "- A weighted average of the prior probabilities is calculated using a list comprehension\n",
    "and the `zip` function.\n",
    "- The resulting weighted average prior probability is stored in `weighted_avg_prior`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c566411",
   "metadata": {},
   "outputs": [],
   "source": [
    " from pyspark.ml import Pipeline\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"featureVector\")\n",
    "classifier = DecisionTreeClassifier(seed=1234, labelCol=\"Cover_Type\",\n",
    "featuresCol=\"featureVector\",\n",
    "predictionCol=\"prediction\")\n",
    "pipeline = Pipeline(stages=[assembler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08782a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = ParamGridBuilder(). \\\n",
    "addGrid(classifier.impurity, [\"gini\", \"entropy\"]). \\\n",
    "addGrid(classifier.maxDepth, [1, 20]). \\\n",
    "addGrid(classifier.maxBins, [40, 300]). \\\n",
    "addGrid(classifier.minInfoGain, [0.0, 0.05]). \\\n",
    "build()\n",
    "multiclassEval = MulticlassClassificationEvaluator(). \\\n",
    "setLabelCol(\"Cover_Type\"). \\\n",
    "setPredictionCol(\"prediction\"). \\\n",
    "setMetricName(\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fd630",
   "metadata": {},
   "source": [
    "- The `VectorAssembler` and `DecisionTreeClassifier` are imported from\n",
    "`pyspark.ml.feature` and `pyspark.ml.classification`, respectively.\n",
    "- A `Pipeline` is created with stages including the `assembler` and `classifier`.\n",
    "- `ParamGridBuilder` is used to build a parameter grid for hyperparameter tuning of the\n",
    "decision tree classifier with various combinations of `impurity`, `maxDepth`,\n",
    "`maxBins`, and `minInfoGain`.\n",
    "- `MulticlassClassificationEvaluator` is initialized to evaluate the accuracy of the\n",
    "predictions on the \"Cover_Type\" column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0be928",
   "metadata": {},
   "outputs": [],
   "source": [
    " from pyspark.ml.tuning import TrainValidationSplit\n",
    "validator = TrainValidationSplit(seed=1234,\n",
    "estimator=pipeline,\n",
    "evaluator=multiclassEval,\n",
    "estimatorParamMaps=paramGrid,\n",
    "trainRatio=0.9)\n",
    "validator_model = validator.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979007c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "best_model = validator_model.bestModel\n",
    "pprint(best_model.stages[1].extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf81f8",
   "metadata": {},
   "source": [
    "- `TrainValidationSplit` is used for hyperparameter tuning and model selection.\n",
    "- The `estimator` is set to the previously defined `pipeline`.\n",
    "- The `evaluator` is set to `multiclassEval`.\n",
    "- `estimatorParamMaps` is set to the previously defined `paramGrid`.\n",
    "- `trainRatio` is set to 0.9, indicating that 90% of the data will be used for training and\n",
    "10% for validation.\n",
    "- The `fit` method is called on `train_data` to train the model and find the best\n",
    "hyperparameters.\n",
    "- `best_model` retrieves the best model from the `validator_model`.\n",
    "- `pprint` is used to print the parameters of the best decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.sort(reverse=True)\n",
    "print(metrics[0])\n",
    "multiclassEval.evaluate(best_model.transform(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3113a",
   "metadata": {},
   "source": [
    "- The `metrics` list is sorted in descending order.\n",
    "- The highest metric value is printed using `print(metrics[0])`.\n",
    "- The `best_model` is used to transform `test_data`, and the accuracy is evaluated\n",
    "using `multiclassEval`.\n",
    "- The evaluated accuracy is stored in the `accuracy` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "def unencode_one_hot(data):\n",
    "wilderness_cols = ['Wilderness_Area_' + str(i) for i in range(4)]\n",
    "wilderness_assembler = VectorAssembler().\\\n",
    "setInputCols(wilderness_cols).\\\n",
    "setOutputCol(\"wilderness\")\n",
    "unhot_udf = udf(lambda v: v.toArray().tolist().index(1))\n",
    "with_wilderness = wilderness_assembler.transform(data).\\\n",
    "drop(*wilderness_cols).\\\n",
    "withColumn(\"wilderness\", unhot_udf(col(\"wilderness\")).cast(IntegerType()))\n",
    "soil_cols = ['Soil_Type_' + str(i) for i in range(40)]\n",
    "soil_assembler = VectorAssembler().\\\n",
    "setInputCols(soil_cols).\\\n",
    "setOutputCol(\"soil\")\n",
    "with_soil = soil_assembler.\\\n",
    "transform(with_wilderness).\\\n",
    "drop(*soil_cols).\\\n",
    "withColumn(\"soil\", unhot_udf(col(\"soil\")).cast(IntegerType()))\n",
    "return with_soil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc7079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unenc_train_data = unencode_one_hot(train_data)\n",
    "unenc_train_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c89a44",
   "metadata": {},
   "source": [
    "- The function `unencode_one_hot` is defined to convert one-hot encoded features\n",
    "back to categorical features.\n",
    "- `wilderness_cols` and `soil_cols` are defined to specify the one-hot encoded\n",
    "columns for wilderness and soil types, respectively.\n",
    "- `VectorAssembler` is used to assemble the one-hot encoded columns into a single\n",
    "vector column for wilderness and soil.\n",
    "- A `udf` (User Defined Function) `unhot_udf` is defined to convert the assembled\n",
    "vectors back to categorical values.\n",
    "- The `transform` method is used to transform the data, drop the original one-hot\n",
    "encoded columns, and add the new categorical columns \"wilderness\" and \"soil\".\n",
    "- The schema of `unenc_train_data` is printed to show the updated columns.\n",
    "-The data is grouped, occurrences of each type is counted and the counts are\n",
    "displayed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "cols = unenc_train_data.columns\n",
    "input_cols = [c for c in cols if c!='Cover_Type']\n",
    "assembler = VectorAssembler().setInputCols(input_cols).\n",
    ",→setOutputCol(\"featureVector\")\n",
    "indexer = VectorIndexer().\\\n",
    "setMaxCategories(40).\\\n",
    "setInputCol(\"featureVector\").setOutputCol(\"indexedVector\")\n",
    "classifier = DecisionTreeClassifier().setLabelCol(\"Cover_Type\").\\\n",
    "setFeaturesCol(\"indexedVector\").\\\n",
    "setPredictionCol(\"prediction\")\n",
    "pipeline = Pipeline().setStages([assembler, indexer, classifier])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b8bda",
   "metadata": {},
   "source": [
    "- The `VectorAssembler` is initialized to assemble all input columns into a single\n",
    "vector column named \"featureVector\".\n",
    "- The `VectorIndexer` is used to automatically identify categorical features and index\n",
    "them.\n",
    "- `setMaxCategories(40)` specifies that features with more than 40 distinct values\n",
    "should be treated as continuous.\n",
    "- The `DecisionTreeClassifier` is set with the label column, features column, and\n",
    "prediction column.\n",
    "- A `Pipeline` is created with stages including the `assembler`, `indexer`, and\n",
    "`classifier`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879456f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unenc_test_data = unencode_one_hot(test_data)\n",
    "best_model.transform(unenc_test_data.drop(\"Cover_Type\")).\\\n",
    "select(\"prediction\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c428090",
   "metadata": {},
   "source": [
    "- The `unencode_one_hot` function is used to preprocess the `test_data` similar to\n",
    "the `train_data`.\n",
    "- The `best_model` is used to transform the preprocessed `unenc_test_data` after\n",
    "dropping the \"Cover_Type\" column.\n",
    "- The `select(\"prediction\")` method selects and displays the predicted \"prediction\" for\n",
    "the first row in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8173b815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
