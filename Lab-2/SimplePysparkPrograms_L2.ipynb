{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFfMKvL8eFgKr7Jlg2N9/v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dev0419/BDA_Lab/blob/main/Lab-2/SimplePysparkPrograms_L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Implement a PySpark script that applies transformations like filter and withColumn on a DataFrame."
      ],
      "metadata": {
        "id": "UxvZpBIjjvhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Basics').getOrCreate()\n",
        "data = [(1, 'John', 'Engineer', 1000),\n",
        "        (2, 'Alice', 'Manager', 2000),\n",
        "        (3, 'Bob', 'Developer', 1500),\n",
        "        (4, 'Jane', 'Manager', 2500),\n",
        "        (5, 'Eve', 'CEO', 5000)]\n",
        "df = spark.createDataFrame(data,schema=['id','name','role','salary'])\n",
        "df.show()\n",
        "\n",
        "#map function\n",
        "def square_salary(salary):\n",
        "    return salary*salary\n",
        "\n",
        "cols = ['id','name','role','squared_salary']\n",
        "map_df = df.rdd.map(lambda row: (row.id,row.name,row.role,square_salary(row.salary))).toDF(cols)\n",
        "print(\"map\")\n",
        "map_df.show()\n",
        "\n",
        "#filter\n",
        "filter_df = df.filter(df.role == 'Manager')\n",
        "print(\"filter\")\n",
        "filter_df.show()\n",
        "\n",
        "#groupby\n",
        "group_df = df.groupBy('role').agg({'salary':'avg'})\n",
        "print(\"group df\")\n",
        "group_df.show()\n",
        "\n",
        "#join\n",
        "\n",
        "dept_data = df.select(['id','role'])\n",
        "print(\"department data\")\n",
        "dept_data.show()\n",
        "join_df = df.join(dept_data.withColumnRenamed('role', 'dept_role'), on='id', how='inner')\n",
        "print(\"join df\")\n",
        "join_df.show()\n",
        "\n",
        "#union must have same no of columns\n",
        "data2 = [(6, 'Tom', 'Analyst', 3000),\n",
        "         (7, 'Mary', 'Developer', 2800),\n",
        "         (8, 'Chris', 'Manager', 3500),\n",
        "         (9, 'Linda', 'Engineer', 3200),\n",
        "         (10, 'Mike', 'CEO', 6000)]\n",
        "\n",
        "df2 = spark.createDataFrame(data2, ['id', 'name', 'role', 'salary'])\n",
        "df2.show()\n",
        "union_df = df.union(df2)\n",
        "print(\"union df\")\n",
        "union_df.show()\n",
        "\n",
        "distinct_df = union_df.distinct()\n",
        "print(\"distinct_df\")\n",
        "distinct_df.show()\n",
        "\n",
        "words_rdd = spark.sparkContext.parallelize(['Hello World', 'How are you'])\n",
        "flat_mapped_rdd = words_rdd.flatMap(lambda x: x.split())\n",
        "print(\"FlatMapped RDD:\")\n",
        "print(flat_mapped_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WLZhOBuj4W7",
        "outputId": "ca155a6e-94c2-4aa7-9f64-270ebe815818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---------+------+\n",
            "| id| name|     role|salary|\n",
            "+---+-----+---------+------+\n",
            "|  1| John| Engineer|  1000|\n",
            "|  2|Alice|  Manager|  2000|\n",
            "|  3|  Bob|Developer|  1500|\n",
            "|  4| Jane|  Manager|  2500|\n",
            "|  5|  Eve|      CEO|  5000|\n",
            "+---+-----+---------+------+\n",
            "\n",
            "map\n",
            "+---+-----+---------+--------------+\n",
            "| id| name|     role|squared_salary|\n",
            "+---+-----+---------+--------------+\n",
            "|  1| John| Engineer|       1000000|\n",
            "|  2|Alice|  Manager|       4000000|\n",
            "|  3|  Bob|Developer|       2250000|\n",
            "|  4| Jane|  Manager|       6250000|\n",
            "|  5|  Eve|      CEO|      25000000|\n",
            "+---+-----+---------+--------------+\n",
            "\n",
            "filter\n",
            "+---+-----+-------+------+\n",
            "| id| name|   role|salary|\n",
            "+---+-----+-------+------+\n",
            "|  2|Alice|Manager|  2000|\n",
            "|  4| Jane|Manager|  2500|\n",
            "+---+-----+-------+------+\n",
            "\n",
            "group df\n",
            "+---------+-----------+\n",
            "|     role|avg(salary)|\n",
            "+---------+-----------+\n",
            "| Engineer|     1000.0|\n",
            "|  Manager|     2250.0|\n",
            "|Developer|     1500.0|\n",
            "|      CEO|     5000.0|\n",
            "+---------+-----------+\n",
            "\n",
            "department data\n",
            "+---+---------+\n",
            "| id|     role|\n",
            "+---+---------+\n",
            "|  1| Engineer|\n",
            "|  2|  Manager|\n",
            "|  3|Developer|\n",
            "|  4|  Manager|\n",
            "|  5|      CEO|\n",
            "+---+---------+\n",
            "\n",
            "join df\n",
            "+---+-----+---------+------+---------+\n",
            "| id| name|     role|salary|dept_role|\n",
            "+---+-----+---------+------+---------+\n",
            "|  1| John| Engineer|  1000| Engineer|\n",
            "|  2|Alice|  Manager|  2000|  Manager|\n",
            "|  3|  Bob|Developer|  1500|Developer|\n",
            "|  4| Jane|  Manager|  2500|  Manager|\n",
            "|  5|  Eve|      CEO|  5000|      CEO|\n",
            "+---+-----+---------+------+---------+\n",
            "\n",
            "+---+-----+---------+------+\n",
            "| id| name|     role|salary|\n",
            "+---+-----+---------+------+\n",
            "|  6|  Tom|  Analyst|  3000|\n",
            "|  7| Mary|Developer|  2800|\n",
            "|  8|Chris|  Manager|  3500|\n",
            "|  9|Linda| Engineer|  3200|\n",
            "| 10| Mike|      CEO|  6000|\n",
            "+---+-----+---------+------+\n",
            "\n",
            "union df\n",
            "+---+-----+---------+------+\n",
            "| id| name|     role|salary|\n",
            "+---+-----+---------+------+\n",
            "|  1| John| Engineer|  1000|\n",
            "|  2|Alice|  Manager|  2000|\n",
            "|  3|  Bob|Developer|  1500|\n",
            "|  4| Jane|  Manager|  2500|\n",
            "|  5|  Eve|      CEO|  5000|\n",
            "|  6|  Tom|  Analyst|  3000|\n",
            "|  7| Mary|Developer|  2800|\n",
            "|  8|Chris|  Manager|  3500|\n",
            "|  9|Linda| Engineer|  3200|\n",
            "| 10| Mike|      CEO|  6000|\n",
            "+---+-----+---------+------+\n",
            "\n",
            "distinct_df\n",
            "+---+-----+---------+------+\n",
            "| id| name|     role|salary|\n",
            "+---+-----+---------+------+\n",
            "|  2|Alice|  Manager|  2000|\n",
            "|  1| John| Engineer|  1000|\n",
            "|  5|  Eve|      CEO|  5000|\n",
            "|  3|  Bob|Developer|  1500|\n",
            "|  4| Jane|  Manager|  2500|\n",
            "|  7| Mary|Developer|  2800|\n",
            "|  6|  Tom|  Analyst|  3000|\n",
            "| 10| Mike|      CEO|  6000|\n",
            "|  8|Chris|  Manager|  3500|\n",
            "|  9|Linda| Engineer|  3200|\n",
            "+---+-----+---------+------+\n",
            "\n",
            "FlatMapped RDD:\n",
            "['Hello', 'World', 'How', 'are', 'you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a PySpark script that performs actions like count and show on a DataFrame."
      ],
      "metadata": {
        "id": "7ShJ2SNgvrHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count\n",
        "df.show()\n",
        "df.count()\n",
        "df.select(count(df.name).alias('name_count'),count(df.role).alias('role_count')).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acbZgZnWvvCV",
        "outputId": "24256c7d-ed6d-4c4f-fe40-4b061d4710f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+---------+------+\n",
            "| id| name|     role|salary|\n",
            "+---+-----+---------+------+\n",
            "|  1| John| Engineer|  1000|\n",
            "|  2|Alice|  Manager|  2000|\n",
            "|  3|  Bob|Developer|  1500|\n",
            "|  4| Jane|  Manager|  2500|\n",
            "|  5|  Eve|      CEO|  5000|\n",
            "+---+-----+---------+------+\n",
            "\n",
            "+----------+----------+\n",
            "|name_count|role_count|\n",
            "+----------+----------+\n",
            "|         5|         5|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Demonstrate how to perform basic aggregations (e.g., sum, average) on a PySpark DataFrame."
      ],
      "metadata": {
        "id": "H2EYrUUBwmyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_df = df.groupBy('role').agg({'salary':'sum'})\n",
        "sum_df.show()\n",
        "avg_df = df.groupBy('role').agg({'salary':'avg'})\n",
        "avg_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqrVKIudwxqg",
        "outputId": "92c2ba7e-9f11-4239-80f8-30339285b7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+\n",
            "|     role|sum(salary)|\n",
            "+---------+-----------+\n",
            "| Engineer|       1000|\n",
            "|  Manager|       4500|\n",
            "|Developer|       1500|\n",
            "|      CEO|       5000|\n",
            "+---------+-----------+\n",
            "\n",
            "+---------+-----------+\n",
            "|     role|avg(salary)|\n",
            "+---------+-----------+\n",
            "| Engineer|     1000.0|\n",
            "|  Manager|     2250.0|\n",
            "|Developer|     1500.0|\n",
            "|      CEO|     5000.0|\n",
            "+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Show how to write a PySpark DataFrame to a CSV file."
      ],
      "metadata": {
        "id": "87oJHqy4yGck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.csv('output.csv', header=True)"
      ],
      "metadata": {
        "id": "jftV4UjdyLhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Implement wordcount program in PySpark."
      ],
      "metadata": {
        "id": "Vt2n-kPkyVM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "data = [\"Hello Spark\", \"How are you Spark\", \"Spark is awesome\"]\n",
        "lines_rdd = spark.sparkContext.parallelize(data)\n",
        "word_counts = lines_rdd.flatMap(lambda line: line.split()) \\\n",
        "                       .map(lambda word: (word, 1)) \\\n",
        "                       .reduceByKey(add)\n",
        "print(\"Word counts:\")\n",
        "for word, count in word_counts.collect():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slPIgdFmyXeN",
        "outputId": "c0843b17-1b18-4825-c435-1e10ffbce63d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word counts:\n",
            "Hello: 1\n",
            "Spark: 3\n",
            "are: 1\n",
            "is: 1\n",
            "awesome: 1\n",
            "How: 1\n",
            "you: 1\n"
          ]
        }
      ]
    }
  ]
}